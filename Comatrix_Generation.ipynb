{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "3PEmIPhmx44V",
        "vzUWDF3h6E6g"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "##### Imports:"
      ],
      "metadata": {
        "id": "8uOW3gLgxTDB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6gEnXAO9wzZI"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pds\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "from numpy import save\n",
        "from google.colab import drive"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Download the Kaggle common nouns dataset and a large corpus of Wikipedia sentences:"
      ],
      "metadata": {
        "id": "hhn_R7TjxWqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install kaggle --upgrade # for kaggle download\n",
        "!pip install -U sentence-transformers # for SBERT pre-trained download\n",
        "\n",
        "# downloading kaggle noun dataset\n",
        "os.environ['KAGGLE_USERNAME'] = ''\n",
        "os.environ['KAGGLE_KEY'] = ''\n",
        "!kaggle datasets download -d mikeortman/wikipedia-sentences # https://www.kaggle.com/datasets/mikeortman/wikipedia-sentences\n",
        "!kaggle datasets download -d leite0407/list-of-nouns # https://www.kaggle.com/datasets/leite0407/list-of-nouns\n",
        "!unzip wikipedia-sentences.zip\n",
        "!unzip list-of-nouns.zip"
      ],
      "metadata": {
        "id": "k8BiLaGaxcp3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Clean the data:"
      ],
      "metadata": {
        "id": "-JGmu1DDxfB7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentences=pds.read_csv('wikisent2.txt', error_bad_lines=False).to_numpy()\n",
        "nouns = pds.read_csv('nounlist.csv').values.tolist()\n",
        "for i in range(len(nouns)):\n",
        "  nouns[i] = nouns[i][0]"
      ],
      "metadata": {
        "id": "bpyLH-G4xi-o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Create a words dict to map words to indicies on the comatrix:"
      ],
      "metadata": {
        "id": "5nitH2-BxwM9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "words_dict = {}\n",
        "for i in range(len(nouns)):\n",
        "  words_dict.update({nouns[i].lower(): i})"
      ],
      "metadata": {
        "id": "ChqfIwcvx0a3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Create the comatrix (v 1.0, only look at next word):"
      ],
      "metadata": {
        "id": "3PEmIPhmx44V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def update_co_mat(co_mat, wrd_list):\n",
        "    # Get all the words in the sentence and store it in an array wrd_lst\n",
        "    for i in range(0, len(wrd_list) - 1): # can increase window size later\n",
        "      co_mat[words_dict[wrd_list[i]]][words_dict[wrd_list[i+1]]] += 1\n",
        "      co_mat[words_dict[wrd_list[i+1]]][words_dict[wrd_list[i]]] += 1\n",
        "\n",
        "\n",
        "# make the matrix\n",
        "co_mat = np.zeros((len(nouns),len(nouns)))\n",
        "counter = 0\n",
        "for s in sentences:\n",
        "  s2 = s[0]\n",
        "  temp_nouns_sent = \"\"\n",
        "  if(not s2[0:1].isdigit()):\n",
        "    for word in s2.split(): # processing at sentence level, could also do all of them without breaking by sentence\n",
        "    # ignore all sentences with digits\n",
        "      if word in nouns:\n",
        "        temp_nouns_sent += word.lower() + \" \"\n",
        "    wrd_list = temp_nouns_sent.split(' ')\n",
        "    wrd_list.pop() # remove the empty element at the end\n",
        "\n",
        "    if(wrd_list != [] and wrd_list != None):\n",
        "      update_co_mat(co_mat, wrd_list)\n",
        "      if(counter%700000==0):\n",
        "        print(str((counter/7000000)*100)+\"%\")\n",
        "    counter+=1"
      ],
      "metadata": {
        "id": "ZEDIicEkx748"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Create comatrix v2.0, look at all nouns in the sentence and scale relevance by distance using (1/2)^n for n words away"
      ],
      "metadata": {
        "id": "vzUWDF3h6E6g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def update_co_mat(co_mat, wrd_list):\n",
        "    # Get all the words in the sentence and store it in an array wrd_lst\n",
        "    for i in range(0, len(wrd_list) - 1): # can increase window size later\n",
        "      for j in range(i + 1, len(wrd_list) - 1):\n",
        "        tempVal = j - i\n",
        "        co_mat[words_dict[wrd_list[i]]][words_dict[wrd_list[j]]] += (1/2)**tempVal\n",
        "        co_mat[words_dict[wrd_list[j]]][words_dict[wrd_list[i]]] += (1/2)**tempVal\n",
        "\n",
        "\n",
        "\n",
        "# make the matrix\n",
        "co_mat = np.zeros((len(nouns),len(nouns)))\n",
        "counter = 0\n",
        "for s in sentences:\n",
        "  s2 = s[0]\n",
        "  temp_nouns_sent = \"\"\n",
        "  if(not s2[0:1].isdigit()):\n",
        "    for word in s2.split(): # processing at sentence level, could also do all of them without breaking by sentence\n",
        "    # ignore all sentences with digits\n",
        "      if word in nouns:\n",
        "        temp_nouns_sent += word.lower() + \" \"\n",
        "    wrd_list = temp_nouns_sent.split(' ')\n",
        "    wrd_list.pop() # remove the empty element at the end\n",
        "\n",
        "    if(wrd_list != [] and wrd_list != None):\n",
        "      update_co_mat(co_mat, wrd_list)\n",
        "      if(counter%700000==0):\n",
        "        print(str((counter/7000000)*100)+\"%\")\n",
        "    counter+=1"
      ],
      "metadata": {
        "id": "Z0ZVdoFA6B5R",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a9e37893-a0c2-42db-9f9a-be8b4de6407e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "20.0%\n",
            "50.0%\n",
            "70.0%\n",
            "80.0%\n",
            "90.0%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Create comatrix v3.0, look at all nouns in the sentence and scale relevance by distance using (1/n) for n words away"
      ],
      "metadata": {
        "id": "0Z-65qUCy_we"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def update_co_mat(co_mat, wrd_list):\n",
        "    # Get all the words in the sentence and store it in an array wrd_lst\n",
        "    for i in range(0, len(wrd_list) - 1): # can increase window size later\n",
        "      for j in range(i + 1, len(wrd_list) - 1):\n",
        "        tempVal = j - i\n",
        "        co_mat[words_dict[wrd_list[i]]][words_dict[wrd_list[j]]] += (1/tempVal)\n",
        "        co_mat[words_dict[wrd_list[j]]][words_dict[wrd_list[i]]] += (1/tempVal)\n",
        "\n",
        "\n",
        "\n",
        "# make the matrix\n",
        "co_mat = np.zeros((len(nouns),len(nouns)))\n",
        "counter = 0\n",
        "for s in sentences:\n",
        "  s2 = s[0]\n",
        "  temp_nouns_sent = \"\"\n",
        "  if(not s2[0:1].isdigit()):\n",
        "    for word in s2.split(): # processing at sentence level, could also do all of them without breaking by sentence\n",
        "    # ignore all sentences with digits\n",
        "      if word in nouns:\n",
        "        temp_nouns_sent += word.lower() + \" \"\n",
        "    wrd_list = temp_nouns_sent.split(' ')\n",
        "    wrd_list.pop() # remove the empty element at the end\n",
        "\n",
        "    if(wrd_list != [] and wrd_list != None):\n",
        "      update_co_mat(co_mat, wrd_list)\n",
        "      if(counter%700000==0):\n",
        "        print(str((counter/7000000)*100)+\"%\")\n",
        "    counter+=1"
      ],
      "metadata": {
        "id": "gBZGEgoZy_TD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test the comatrix:"
      ],
      "metadata": {
        "id": "Ch1ILbsryC5D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Co-occurrence Matrix: \")\n",
        "print(co_mat.shape)\n",
        "\n",
        "words = [\"water\", \"bottle\", \"river\", \"dog\", \"park\", \"tree\"]\n",
        "print(co_mat[words_dict[words[0]]][words_dict[words[1]]])\n",
        "print(co_mat[words_dict[words[0]]][words_dict[words[2]]])\n",
        "print(co_mat[words_dict[words[1]]][words_dict[words[2]]])\n",
        "print()\n",
        "print(co_mat[words_dict[words[3]]][words_dict[words[4]]])\n",
        "print(co_mat[words_dict[words[3]]][words_dict[words[5]]])\n",
        "print(co_mat[words_dict[words[4]]][words_dict[words[5]]])"
      ],
      "metadata": {
        "id": "JeyuVUxSyERC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Save the comatrix to Google Drive:"
      ],
      "metadata": {
        "id": "SiYquu9DyOqy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive')\n",
        "\n",
        "with open('/content/drive/My Drive/comatrix3.npy', 'wb') as f:\n",
        "  np.save(f, co_mat)"
      ],
      "metadata": {
        "id": "l-KcZVAEyLOx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/drive/My Drive/J-Term 2023/comatrix3.npy', 'rb') as f: \n",
        "  co_mat=np.load(f, allow_pickle=True)\n",
        "\n",
        "reduced = np.array([x[:10] for x in co_mat], dtype=float)\n",
        "\n",
        "words_dict_rev = {}\n",
        "for i in range(len(nouns)):\n",
        "  words_dict.update({i:nouns[i].lower()})\n",
        "\n",
        "df = pds.DataFrame(co_mat).rename(columns=words_dict, index=words_dict)\n",
        "print(df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xhEzg70SHTel",
        "outputId": "a042d2ff-c215-4e5b-dad1-c93b9f11a64b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "                   cd       suv          tv  aardvark  abacus  abbey  \\\n",
            "cd         113.074607  0.000000    2.833333       0.0     0.0    0.0   \n",
            "suv          0.000000  0.666667    0.000000       0.0     0.0    0.0   \n",
            "tv           2.833333  0.000000  374.113051       0.0     0.0    0.0   \n",
            "aardvark     0.000000  0.000000    0.000000       0.0     0.0    0.0   \n",
            "abacus       0.000000  0.000000    0.000000       0.0     0.0    0.0   \n",
            "...               ...       ...         ...       ...     ...    ...   \n",
            "zoo          0.000000  0.000000    0.333333       0.0     0.0    0.0   \n",
            "zoologist    0.000000  0.000000    0.000000       0.0     0.0    0.0   \n",
            "zoology      0.000000  0.000000    0.000000       0.0     0.0    0.0   \n",
            "zoot-suit    0.000000  0.000000    0.000000       0.0     0.0    0.0   \n",
            "zucchini     0.000000  0.000000    0.000000       0.0     0.0    0.0   \n",
            "\n",
            "           abbreviation  abdomen   ability  abnormality  ...  zinc  zipper  \\\n",
            "cd                 0.75      0.0  0.000000          0.0  ...   0.0     0.0   \n",
            "suv                0.00      0.0  0.000000          0.0  ...   0.0     0.0   \n",
            "tv                 0.00      0.0  0.000000          0.0  ...   0.0     0.0   \n",
            "aardvark           0.00      0.0  0.000000          0.0  ...   0.0     0.0   \n",
            "abacus             0.00      0.0  0.000000          0.0  ...   0.0     0.0   \n",
            "...                 ...      ...       ...          ...  ...   ...     ...   \n",
            "zoo                0.00      0.0  0.023841          0.0  ...   0.0     0.0   \n",
            "zoologist          0.00      0.0  0.000000          0.0  ...   0.0     0.0   \n",
            "zoology            0.00      0.0  0.000000          0.0  ...   0.0     0.0   \n",
            "zoot-suit          0.00      0.0  0.000000          0.0  ...   0.0     0.0   \n",
            "zucchini           0.00      0.0  0.000000          0.0  ...   0.0     0.0   \n",
            "\n",
            "           zither  zombie      zone       zoo  zoologist  zoology  zoot-suit  \\\n",
            "cd            0.0     0.0  0.000000  0.000000        0.0      0.0        0.0   \n",
            "suv           0.0     0.0  0.000000  0.000000        0.0      0.0        0.0   \n",
            "tv            0.0     0.0  0.333333  0.333333        0.0      0.0        0.0   \n",
            "aardvark      0.0     0.0  0.000000  0.000000        0.0      0.0        0.0   \n",
            "abacus        0.0     0.0  0.000000  0.000000        0.0      0.0        0.0   \n",
            "...           ...     ...       ...       ...        ...      ...        ...   \n",
            "zoo           0.0     0.0  0.004926  1.000000        0.0      0.0        0.0   \n",
            "zoologist     0.0     0.0  0.547619  0.000000        0.0      0.0        0.0   \n",
            "zoology       0.0     0.0  0.000000  0.000000        0.0      0.0        0.0   \n",
            "zoot-suit     0.0     0.0  0.000000  0.000000        0.0      0.0        0.0   \n",
            "zucchini      0.0     0.0  0.000000  0.000000        0.0      0.0        0.0   \n",
            "\n",
            "           zucchini  \n",
            "cd              0.0  \n",
            "suv             0.0  \n",
            "tv              0.0  \n",
            "aardvark        0.0  \n",
            "abacus          0.0  \n",
            "...             ...  \n",
            "zoo             0.0  \n",
            "zoologist       0.0  \n",
            "zoology         0.0  \n",
            "zoot-suit       0.0  \n",
            "zucchini        0.0  \n",
            "\n",
            "[6800 rows x 6800 columns]\n"
          ]
        }
      ]
    }
  ]
}